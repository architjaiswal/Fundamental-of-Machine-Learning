{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Assignment 2 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crab Dataset Description\n",
    "\n",
    "**The Crab Data Set has 200 samples and 7 features (Frontal Lip, Rear Width, Length, Width, Depth, Male and Female), describing 5 morphological measurements on 50 crabs each of two color forms and both sexes, of the species *Leptograpsus* variegatus collected at Fremantle, W. Australia.**\n",
    "\n",
    "* **Dataset Source: Campbell, N.A. and Mahon, R.J. (1974) A multivariate study of variation in two species of rock crab of genus *Leptograpsus*. *Australian Journal of Zoology* 22, 417â€“425.**\n",
    "\n",
    "**The data set is saved in the file \"crab.txt\": the firt column corresponds to the class label (crab species) and the other 7 columns correspond to the features.**\n",
    "\n",
    "**Use the first 140 samples as your training set and the last 60 samples as your test set.**\n",
    "\n",
    "## Problem Set\n",
    "\n",
    "**Answer the following questions:**\n",
    "\n",
    "1. **Implement the Naive Bayes classifier, under the assumption that your data likelihood model $p(x|C_j)$ is a multivariate Gaussian and the prior probabilities $p(C_j)$ are dictated by the number of samples $n_j\\in\\mathbb{R}$ that you have for each class. Build your own code to implement the classifier.**\n",
    "\n",
    "2. **Did you encounter any problems when implementing the probabilistic generative model? What is your solution for the problem? Explain why your solution works. (Note: There is more than one solution.)**\n",
    "\n",
    "3. **Report your classification results in terms of a confusion matrix in both training and test set. (You can use the function ```confusion_matrix``` from the module ```sklearn.metrics```.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>FrontalLip</th>\n",
       "      <th>RearWidth</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Male</th>\n",
       "      <th>Female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20.6</td>\n",
       "      <td>14.4</td>\n",
       "      <td>42.8</td>\n",
       "      <td>46.5</td>\n",
       "      <td>19.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.3</td>\n",
       "      <td>11.1</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32.3</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16.7</td>\n",
       "      <td>14.3</td>\n",
       "      <td>32.3</td>\n",
       "      <td>37.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9.8</td>\n",
       "      <td>8.9</td>\n",
       "      <td>20.4</td>\n",
       "      <td>23.9</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>14.1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.5</td>\n",
       "      <td>13.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Species  FrontalLip  RearWidth  Length  Width  Depth  Male  Female\n",
       "0        0        20.6       14.4    42.8   46.5   19.6     1       0\n",
       "1        1        13.3       11.1    27.8   32.3   11.3     1       0\n",
       "2        0        16.7       14.3    32.3   37.0   14.7     0       1\n",
       "3        1         9.8        8.9    20.4   23.9    8.8     0       1\n",
       "4        0        15.6       14.1    31.0   34.5   13.8     0       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"crab.txt\", delimiter=\"\\t\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((140, 7), (60, 7), (140,), (60,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Partitioning the data into training and test sets\n",
    "\n",
    "X_train = data.iloc[:140,1:].to_numpy()\n",
    "t_train = data.iloc[:140,0].to_numpy()\n",
    "\n",
    "X_test = data.iloc[140:,1:].to_numpy()\n",
    "t_test = data.iloc[140:,0].to_numpy()\n",
    "\n",
    "X_train.shape, X_test.shape, t_train.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(remainder='passthrough',\n",
       "                  transformers=[('num_attribs', StandardScaler(),\n",
       "                                 [0, 1, 2, 3, 4])])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This pipeline will apply Standardization to all numerical attributes \n",
    "# The attributes that are one-hot/interger-encoded (such as gender) will remain as is\n",
    "\n",
    "scaling_pipeline = ColumnTransformer([('num_attribs', StandardScaler(), list(range(5)))],\n",
    "                                    remainder='passthrough')\n",
    "\n",
    "scaling_pipeline.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaling_pipeline.transform(X_train)\n",
    "X_test = scaling_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Naive Bayes Classifier - estimating its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5142857142857142, 0.4857142857142857)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prior probabilities\n",
    "\n",
    "pC0 = np.sum(t_train==0)/t_train.size\n",
    "pC1 = np.sum(t_train==1)/t_train.size\n",
    "\n",
    "pC0, pC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means and covariances of the data likelihood\n",
    "\n",
    "mu0 = np.mean(X_train[t_train==0,:],axis=0)\n",
    "mu1 = np.mean(X_train[t_train==1,:],axis=0)\n",
    "\n",
    "cov0 = np.cov(X_train[t_train==0,:].T)\n",
    "cov1 = np.cov(X_train[t_train==1,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training Data Likelihood\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y0_train \u001b[38;5;241m=\u001b[39m \u001b[43mmultivariate_normal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcov0\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#P(x|C0)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y1_train \u001b[38;5;241m=\u001b[39m multivariate_normal\u001b[38;5;241m.\u001b[39mpdf(X_train, mean\u001b[38;5;241m=\u001b[39mmu1, cov\u001b[38;5;241m=\u001b[39mcov1) \u001b[38;5;66;03m#P(x|C1)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Test Data Likelihood\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eee3773\\lib\\site-packages\\scipy\\stats\\_multivariate.py:516\u001b[0m, in \u001b[0;36mmultivariate_normal_gen.pdf\u001b[1;34m(self, x, mean, cov, allow_singular)\u001b[0m\n\u001b[0;32m    514\u001b[0m dim, mean, cov \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_parameters(\u001b[38;5;28;01mNone\u001b[39;00m, mean, cov)\n\u001b[0;32m    515\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_quantiles(x, dim)\n\u001b[1;32m--> 516\u001b[0m psd \u001b[38;5;241m=\u001b[39m \u001b[43m_PSD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_singular\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    517\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logpdf(x, mean, psd\u001b[38;5;241m.\u001b[39mU, psd\u001b[38;5;241m.\u001b[39mlog_pdet, psd\u001b[38;5;241m.\u001b[39mrank))\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _squeeze_output(out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\eee3773\\lib\\site-packages\\scipy\\stats\\_multivariate.py:165\u001b[0m, in \u001b[0;36m_PSD.__init__\u001b[1;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[0;32m    163\u001b[0m d \u001b[38;5;241m=\u001b[39m s[s \u001b[38;5;241m>\u001b[39m eps]\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_singular:\n\u001b[1;32m--> 165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mLinAlgError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msingular matrix\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    166\u001b[0m s_pinv \u001b[38;5;241m=\u001b[39m _pinv_1d(s, eps)\n\u001b[0;32m    167\u001b[0m U \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(u, np\u001b[38;5;241m.\u001b[39msqrt(s_pinv))\n",
      "\u001b[1;31mLinAlgError\u001b[0m: singular matrix"
     ]
    }
   ],
   "source": [
    "# Training Data Likelihood\n",
    "y0_train = multivariate_normal.pdf(X_train, mean=mu0, cov=cov0) #P(x|C0)\n",
    "y1_train = multivariate_normal.pdf(X_train, mean=mu1, cov=cov1) #P(x|C1)\n",
    "\n",
    "# Test Data Likelihood\n",
    "y0_test = multivariate_normal.pdf(X_test, mean=mu0, cov=cov0) #P(x|C0)\n",
    "y1_test = multivariate_normal.pdf(X_test, mean=mu1, cov=cov1) #P(x|C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we used all 7 features, the covariance matrix $\\Sigma_X$ would be singular. This is because one of the features is colinear with another feature. In particular, features male and female are the complement of one another.\n",
    "\n",
    "There are 2 ways to address this issue:\n",
    "\n",
    "1. Eliminate one of the features (method used here)\n",
    "2. Diagonally-load the covariance matrix: $\\Sigma_X + \\lambda I$\n",
    "\n",
    "Either method will be accepted equally. But note that if one decides to use method 2 (diagonally load the covariance matrix), then we have an extra hyperparameter we need to estimate the value for, $\\lambda$. It is typically preferred to perform feature selection than distortion the covariance to force it to become full rank (and hence invertible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Feature Selection\n",
    "\n",
    "In this first approach, we simply eliminate the feature/s that are colinear with one another. In this dataset, it is easy for us to see that the features male and female are the complement of one another, knowing if a sample is male, then we know it is not female, and vice-versa. So we can eliminate one of then as keep both is redundant.\n",
    "\n",
    "But, in practice, sometimes this identification is not straightforwardly visible from the data. But we can measure correlation coefficient between features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>FrontalLip</th>\n",
       "      <th>RearWidth</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Male</th>\n",
       "      <th>Female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Species</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-0.437966</td>\n",
       "      <td>-0.315751</td>\n",
       "      <td>-0.288333</td>\n",
       "      <td>-0.216180</td>\n",
       "      <td>-0.423716</td>\n",
       "      <td>-2.664535e-17</td>\n",
       "      <td>2.220446e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrontalLip</th>\n",
       "      <td>-4.379655e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906988</td>\n",
       "      <td>0.978842</td>\n",
       "      <td>0.964956</td>\n",
       "      <td>0.987627</td>\n",
       "      <td>4.330897e-02</td>\n",
       "      <td>-4.330897e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RearWidth</th>\n",
       "      <td>-3.157507e-01</td>\n",
       "      <td>0.906988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892743</td>\n",
       "      <td>0.900402</td>\n",
       "      <td>0.889205</td>\n",
       "      <td>-2.915970e-01</td>\n",
       "      <td>2.915970e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length</th>\n",
       "      <td>-2.883330e-01</td>\n",
       "      <td>0.978842</td>\n",
       "      <td>0.892743</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995023</td>\n",
       "      <td>0.983204</td>\n",
       "      <td>1.049828e-01</td>\n",
       "      <td>-1.049828e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Width</th>\n",
       "      <td>-2.161801e-01</td>\n",
       "      <td>0.964956</td>\n",
       "      <td>0.900402</td>\n",
       "      <td>0.995023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967812</td>\n",
       "      <td>7.443726e-02</td>\n",
       "      <td>-7.443726e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Depth</th>\n",
       "      <td>-4.237165e-01</td>\n",
       "      <td>0.987627</td>\n",
       "      <td>0.889205</td>\n",
       "      <td>0.983204</td>\n",
       "      <td>0.967812</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.971958e-02</td>\n",
       "      <td>-8.971958e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>-2.664535e-17</td>\n",
       "      <td>0.043309</td>\n",
       "      <td>-0.291597</td>\n",
       "      <td>0.104983</td>\n",
       "      <td>0.074437</td>\n",
       "      <td>0.089720</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>2.220446e-17</td>\n",
       "      <td>-0.043309</td>\n",
       "      <td>0.291597</td>\n",
       "      <td>-0.104983</td>\n",
       "      <td>-0.074437</td>\n",
       "      <td>-0.089720</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Species  FrontalLip  RearWidth    Length     Width     Depth  \\\n",
       "Species     1.000000e+00   -0.437966  -0.315751 -0.288333 -0.216180 -0.423716   \n",
       "FrontalLip -4.379655e-01    1.000000   0.906988  0.978842  0.964956  0.987627   \n",
       "RearWidth  -3.157507e-01    0.906988   1.000000  0.892743  0.900402  0.889205   \n",
       "Length     -2.883330e-01    0.978842   0.892743  1.000000  0.995023  0.983204   \n",
       "Width      -2.161801e-01    0.964956   0.900402  0.995023  1.000000  0.967812   \n",
       "Depth      -4.237165e-01    0.987627   0.889205  0.983204  0.967812  1.000000   \n",
       "Male       -2.664535e-17    0.043309  -0.291597  0.104983  0.074437  0.089720   \n",
       "Female      2.220446e-17   -0.043309   0.291597 -0.104983 -0.074437 -0.089720   \n",
       "\n",
       "                    Male        Female  \n",
       "Species    -2.664535e-17  2.220446e-17  \n",
       "FrontalLip  4.330897e-02 -4.330897e-02  \n",
       "RearWidth  -2.915970e-01  2.915970e-01  \n",
       "Length      1.049828e-01 -1.049828e-01  \n",
       "Width       7.443726e-02 -7.443726e-02  \n",
       "Depth       8.971958e-02 -8.971958e-02  \n",
       "Male        1.000000e+00 -1.000000e+00  \n",
       "Female     -1.000000e+00  1.000000e+00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is clear that the feature female and male are co-linear.\n",
    "\n",
    "* We can also see that other features are highly correlated with others (e.g. length-and-width, length-and-depth).\n",
    "* In practice we can decide to eliminate them or use, for example, Principal Component Analysis (PCA) to decorrelate the features. We will see how to do this later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((140, 6), (60, 6), (140,), (60,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminating the last feature (\"female\")\n",
    "\n",
    "X_train = data.iloc[:140,1:7].to_numpy()\n",
    "X_test = data.iloc[140:,1:7].to_numpy()\n",
    "t_train = data.iloc[:140,0].to_numpy()\n",
    "t_test = data.iloc[140:,0].to_numpy()\n",
    "\n",
    "X_train.shape, X_test.shape, t_train.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_pipeline.fit(X_train)\n",
    "\n",
    "X_train = scaling_pipeline.transform(X_train)\n",
    "X_test = scaling_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recomputing MLE estimates for mean and covariance\n",
    "\n",
    "mu0 = np.mean(X_train[t_train==0,:],axis=0)\n",
    "mu1 = np.mean(X_train[t_train==1,:],axis=0)\n",
    "\n",
    "cov0 = np.cov(X_train[t_train==0,:].T)\n",
    "cov1 = np.cov(X_train[t_train==1,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((140,), (140,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Data Likelihood\n",
    "y0_train = multivariate_normal.pdf(X_train, mean=mu0, cov=cov0) #P(x|C0)\n",
    "y1_train = multivariate_normal.pdf(X_train, mean=mu1, cov=cov1) #P(x|C1)\n",
    "\n",
    "# Test Data Likelihood\n",
    "y0_test = multivariate_normal.pdf(X_test, mean=mu0, cov=cov0) #P(x|C0)\n",
    "y1_test = multivariate_normal.pdf(X_test, mean=mu1, cov=cov1) #P(x|C1)\n",
    "\n",
    "y0_train.shape, y1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior for Training Data\n",
    "pos0_train = (y0_train*pC0)/(y0_train*pC0 + y1_train*pC1) # P(C0|x_train) \n",
    "pos1_train = (y1_train*pC1)/(y0_train*pC0 + y1_train*pC1) # P(C1|x_train) \n",
    "pos_train = np.array([pos0_train, pos1_train]).T # Creating a matrix with posterior probabilities\n",
    "likelihood_train = np.array([y0_train, y1_train]).T # Creating a matrix with likelihoods\n",
    "\n",
    "# Posterior for Test Data\n",
    "pos0_test = (y0_test*pC0)/(y0_test*pC0 + y1_test*pC1) # P(C0|x_test)\n",
    "pos1_test = (y1_test*pC1)/(y0_test*pC0 + y1_test*pC1) # P(C0|x_test) \n",
    "pos_test = np.array([pos0_test, pos1_test]).T # Creating a matrix with posterior probabilities\n",
    "likelihood_test = np.array([y0_test, y1_test]).T # Creating a matrix with likelihoods\n",
    "\n",
    "# Prediction for Training Data\n",
    "predict_train = np.argmax(pos_train, axis=1) # Label prediction for training data\n",
    "# labels it as the class with largest posterior\n",
    "predict_likelihood_train = likelihood_train[predict_train] # Likelihood value for the assigned class\n",
    "\n",
    "# Prediction for Test Data\n",
    "predict_test = np.argmax(pos_test, axis=1) # Label prediction for test set\n",
    "predict_likelihood_test = likelihood_train[predict_test] # Likelihood value for the assigned class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix in Training\n",
      "[[72  0]\n",
      " [ 0 68]]\n",
      "Confusion matrix in Test\n",
      "[[28  0]\n",
      " [ 0 32]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion matrix in Training')\n",
    "print(confusion_matrix(t_train, predict_train))\n",
    "\n",
    "print('Confusion matrix in Test')\n",
    "print(confusion_matrix(t_test, predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix show that all samples were correctly classified for both training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - Diagonally-load the Covariance matrix\n",
    "\n",
    "Recall the multivariate Gaussian probability density function (PDF):\n",
    "\n",
    "\\begin{align*}\n",
    "f(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} \\exp{\\left(-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\mathbf{\\mu})\\right)}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^d$ is a data sample, $\\mathbf{\\mu}\\in\\mathbb{R}^d$ is the mean vector, $\\Sigma$ is the covariance matrix, $|\\Sigma|$ is the determinant of the covariance matrix and $\\Sigma^{-1}$ is the inverse of the covariance matrix.\n",
    "\n",
    "In order for us to implement this PDF, the covariance matrix must be invertible (i.e. its determinant must be different than 0). As we saw from before, at least one feature is linearly dependent on another (female and male features), hence the covariance matrix will have dependent column/s, which will make it not full rank, which in turn will produce a determinant of 0 and it is not invertible.\n",
    "\n",
    "Let's obtain the original data (in the 7-dimensional space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((140, 7), (60, 7), (140,), (60,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Partitioning the data into training and test sets\n",
    "\n",
    "X_train = data.iloc[:140,1:].to_numpy()\n",
    "t_train = data.iloc[:140,0].to_numpy()\n",
    "\n",
    "X_test = data.iloc[140:,1:].to_numpy()\n",
    "t_test = data.iloc[140:,0].to_numpy()\n",
    "\n",
    "X_train.shape, X_test.shape, t_train.shape, t_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_pipeline.fit(X_train)\n",
    "\n",
    "X_train = scaling_pipeline.transform(X_train)\n",
    "X_test = scaling_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.007194</td>\n",
       "      <td>0.922772</td>\n",
       "      <td>0.984102</td>\n",
       "      <td>0.968550</td>\n",
       "      <td>0.995087</td>\n",
       "      <td>0.015013</td>\n",
       "      <td>-0.015013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.922772</td>\n",
       "      <td>1.007194</td>\n",
       "      <td>0.900605</td>\n",
       "      <td>0.905846</td>\n",
       "      <td>0.904054</td>\n",
       "      <td>-0.146339</td>\n",
       "      <td>0.146339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.984102</td>\n",
       "      <td>0.900605</td>\n",
       "      <td>1.007194</td>\n",
       "      <td>1.001785</td>\n",
       "      <td>0.989244</td>\n",
       "      <td>0.050729</td>\n",
       "      <td>-0.050729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.968550</td>\n",
       "      <td>0.905846</td>\n",
       "      <td>1.001785</td>\n",
       "      <td>1.007194</td>\n",
       "      <td>0.972557</td>\n",
       "      <td>0.034288</td>\n",
       "      <td>-0.034288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.995087</td>\n",
       "      <td>0.904054</td>\n",
       "      <td>0.989244</td>\n",
       "      <td>0.972557</td>\n",
       "      <td>1.007194</td>\n",
       "      <td>0.039355</td>\n",
       "      <td>-0.039355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.015013</td>\n",
       "      <td>-0.146339</td>\n",
       "      <td>0.050729</td>\n",
       "      <td>0.034288</td>\n",
       "      <td>0.039355</td>\n",
       "      <td>0.251593</td>\n",
       "      <td>-0.251593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.015013</td>\n",
       "      <td>0.146339</td>\n",
       "      <td>-0.050729</td>\n",
       "      <td>-0.034288</td>\n",
       "      <td>-0.039355</td>\n",
       "      <td>-0.251593</td>\n",
       "      <td>0.251593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6\n",
       "0  1.007194  0.922772  0.984102  0.968550  0.995087  0.015013 -0.015013\n",
       "1  0.922772  1.007194  0.900605  0.905846  0.904054 -0.146339  0.146339\n",
       "2  0.984102  0.900605  1.007194  1.001785  0.989244  0.050729 -0.050729\n",
       "3  0.968550  0.905846  1.001785  1.007194  0.972557  0.034288 -0.034288\n",
       "4  0.995087  0.904054  0.989244  0.972557  1.007194  0.039355 -0.039355\n",
       "5  0.015013 -0.146339  0.050729  0.034288  0.039355  0.251593 -0.251593\n",
       "6 -0.015013  0.146339 -0.050729 -0.034288 -0.039355 -0.251593  0.251593"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the covariance matrix for the training data\n",
    "\n",
    "pd.DataFrame(np.cov(X_train.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the last two columns are linearly dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5142857142857142, 0.4857142857142857)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prior probabilities\n",
    "\n",
    "pC0 = np.sum(t_train==0)/t_train.size\n",
    "pC1 = np.sum(t_train==1)/t_train.size\n",
    "\n",
    "pC0, pC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means and covariances of the data likelihood\n",
    "\n",
    "mu0 = np.mean(X_train[t_train==0,:],axis=0)\n",
    "mu1 = np.mean(X_train[t_train==1,:],axis=0)\n",
    "\n",
    "cov0 = np.cov(X_train[t_train==0,:].T)\n",
    "cov1 = np.cov(X_train[t_train==1,:].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's diagonally-load the covariance before computing the data likelihood for each class, that is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\Sigma} \\leftarrow \\mathbf{\\Sigma} + \\lambda \\mathbf{I}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\lambda$ is a (constant) hyperparameter that must be learned using cross-validation for the training data. I am not going to include that step here but you should implement CV when utilizing this approaches in your research projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagonally-loading covariance matrices -- injecting small value along the diagonal of the covariance. \n",
    "# this forces it to be full rank, and hence invertible.\n",
    "\n",
    "reg = 0.0001 # this is a tunable parameter, that must be learned using cross-validation\n",
    "\n",
    "cov0_reg = cov0 + reg*np.eye(len(cov0))\n",
    "\n",
    "cov1_reg = cov1 + reg*np.eye(len(cov1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Likelihood\n",
    "y0_train = multivariate_normal.pdf(X_train, mean=mu0, cov=cov0_reg) #P(x|C0)\n",
    "y1_train = multivariate_normal.pdf(X_train, mean=mu1, cov=cov1_reg) #P(x|C1)\n",
    "\n",
    "# Test Data Likelihood\n",
    "y0_test = multivariate_normal.pdf(X_test, mean=mu0, cov=cov0_reg) #P(x|C0)\n",
    "y1_test = multivariate_normal.pdf(X_test, mean=mu1, cov=cov1_reg) #P(x|C1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No *singular* error message this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior for Training Data\n",
    "pos0_train = (y0_train*pC0)/(y0_train*pC0 + y1_train*pC1) # P(C0|x_train) \n",
    "pos1_train = (y1_train*pC1)/(y0_train*pC0 + y1_train*pC1) # P(C1|x_train) \n",
    "pos_train = np.array([pos0_train, pos1_train]).T # Creating a matrix with posterior probabilities\n",
    "likelihood_train = np.array([y0_train, y1_train]).T # Creating a matrix with likelihoods\n",
    "\n",
    "# Posterior for Test Data\n",
    "pos0_test = (y0_test*pC0)/(y0_test*pC0 + y1_test*pC1) # P(C0|x_test)\n",
    "pos1_test = (y1_test*pC1)/(y0_test*pC0 + y1_test*pC1) # P(C0|x_test) \n",
    "pos_test = np.array([pos0_test, pos1_test]).T # Creating a matrix with posterior probabilities\n",
    "likelihood_test = np.array([y0_test, y1_test]).T # Creating a matrix with likelihoods\n",
    "\n",
    "# Prediction for Training Data\n",
    "predict_train = np.argmax(pos_train, axis=1) # Label prediction for training data\n",
    "# labels it as the class with largest posterior\n",
    "predict_likelihood_train = likelihood_train[predict_train] # Likelihood value for the assigned class\n",
    "\n",
    "# Prediction for Test Data\n",
    "predict_test = np.argmax(pos_test, axis=1) # Label prediction for test set\n",
    "predict_likelihood_test = likelihood_train[predict_test] # Likelihood value for the assigned class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix in Training\n",
      "[[72  0]\n",
      " [ 0 68]]\n",
      "Confusion matrix in Test\n",
      "[[28  0]\n",
      " [ 0 32]]\n"
     ]
    }
   ],
   "source": [
    "print('Confusion matrix in Training')\n",
    "print(confusion_matrix(t_train, predict_train))\n",
    "\n",
    "print('Confusion matrix in Test')\n",
    "print(confusion_matrix(t_test, predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix show that all samples were correctly classified for both training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you go back, and use a very large value for $\\lambda$, say $\\lambda=10$, the confusion matrices will display a lot more errors. This large regularizer will distort the data significantly to impact the classification results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
